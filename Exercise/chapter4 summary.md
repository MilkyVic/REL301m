# üìò Ch∆∞∆°ng 4: Dynamic Programming (Sutton & Barto, 2020)

## ‚úÖ M·ª•c ti√™u ch√≠nh
Dynamic Programming (DP) l√† t·∫≠p h·ª£p c√°c thu·∫≠t to√°n gi√∫p gi·∫£i b√†i to√°n t·ªëi ∆∞u ch√≠nh s√°ch trong MDP (Markov Decision Process) khi **bi·∫øt r√µ m√¥ h√¨nh m√¥i tr∆∞·ªùng**. DP l√† c∆° s·ªü l√Ω thuy·∫øt cho nhi·ªÅu ph∆∞∆°ng ph√°p h·ªçc tƒÉng c∆∞·ªùng hi·ªán ƒë·∫°i.

---

## 1. Policy Evaluation (D·ª± ƒëo√°n gi√° tr·ªã ch√≠nh s√°ch)

### M·ª•c ti√™u:
T√≠nh to√°n gi√° tr·ªã k·ª≥ v·ªçng (expected return) c·ªßa t·ª´ng tr·∫°ng th√°i khi th·ª±c hi·ªán ch√≠nh s√°ch $ \pi $. Gi√° tr·ªã n√†y g·ªçi l√† **state-value function**: $ v_\pi(s) $

### C√¥ng th·ª©c k·ª≥ v·ªçng:

$$
v_\pi(s) = \mathbb{E}_\pi [ G_t \mid S_t = s ] = \mathbb{E}_\pi [ R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s ]
$$

- $ G_t $: t·ªïng ph·∫ßn th∆∞·ªüng v·ªÅ sau (return)
- $ \gamma $: h·ªá s·ªë chi·∫øt kh·∫•u
- T·ª©c l√† gi√° tr·ªã c·ªßa m·ªôt tr·∫°ng th√°i b·∫±ng ph·∫ßn th∆∞·ªüng ngay sau ƒë√≥ c·ªông v·ªõi gi√° tr·ªã chi·∫øt kh·∫•u c·ªßa tr·∫°ng th√°i k·∫ø ti·∫øp.

### D·∫°ng t·ªïng qu√°t (Bellman Equation):

$$
v_\pi(s) = \sum_a \pi(a|s) \sum_{s',r} p(s', r \mid s, a) [r + \gamma v_\pi(s')]
\tag{4.4}
$$

Gi·∫£i th√≠ch:
- $ \pi(a|s) $: x√°c su·∫•t ch·ªçn h√†nh ƒë·ªông $ a $ ·ªü tr·∫°ng th√°i $ s $
- $ p(s', r \mid s, a) $: x√°c su·∫•t chuy·ªÉn ƒë·∫øn tr·∫°ng th√°i $ s' $ v√† nh·∫≠n ph·∫ßn th∆∞·ªüng $ r $
- C√¥ng th·ª©c n√†y t√≠nh k·ª≥ v·ªçng theo m·ªçi h√†nh ƒë·ªông v√† m·ªçi k·∫øt qu·∫£ c√≥ th·ªÉ.

### Ph∆∞∆°ng ph√°p l·∫∑p (Iterative Policy Evaluation):

$$
v_{k+1}(s) = \sum_a \pi(a|s) \sum_{s',r} p(s', r \mid s, a) [r + \gamma v_k(s')]
\tag{4.5}
$$

- B·∫Øt ƒë·∫ßu v·ªõi $ v_0(s) $ b·∫•t k·ª≥, l·∫∑p l·∫°i c√¥ng th·ª©c n√†y ƒë·ªÉ h·ªôi t·ª• v·ªÅ $ v_\pi(s) $

---

## 2. Policy Improvement (C·∫£i ti·∫øn ch√≠nh s√°ch)

### M·ª•c ti√™u:
T√¨m ch√≠nh s√°ch t·ªët h∆°n b·∫±ng c√°ch t·∫≠n d·ª•ng $ v_\pi $ ƒë·ªÉ ch·ªçn h√†nh ƒë·ªông t·ªët h∆°n.

### H√†m h√†nh ƒë·ªông (Action-Value Function):

$$
q_\pi(s, a) = \sum_{s', r} p(s', r \mid s, a) [r + \gamma v_\pi(s')]
\tag{4.6}
$$

- Cho bi·∫øt gi√° tr·ªã k·ª≥ v·ªçng n·∫øu ch·ªçn h√†nh ƒë·ªông $ a $ t·∫°i tr·∫°ng th√°i $ s $ v√† sau ƒë√≥ tu√¢n theo ch√≠nh s√°ch $ \pi $

### Ch√≠nh s√°ch tham lam (Greedy policy):

$$
\pi'(s) = \arg\max_a q_\pi(s, a)
\tag{4.9}
$$

- T·∫°i m·ªói tr·∫°ng th√°i, ch·ªçn h√†nh ƒë·ªông c√≥ $ q $ l·ªõn nh·∫•t ‚Üí d·∫´n ƒë·∫øn ch√≠nh s√°ch m·ªõi t·ªët h∆°n ho·∫∑c b·∫±ng ch√≠nh s√°ch c≈©.

---

## 3. Policy Iteration (L·∫∑p c·∫£i ti·∫øn ch√≠nh s√°ch)

### M√¥ t·∫£:
- B·∫Øt ƒë·∫ßu t·ª´ ch√≠nh s√°ch b·∫•t k·ª≥
- L·∫∑p l·∫°i 2 b∆∞·ªõc:
  1. Policy Evaluation: T√≠nh $ v_\pi $
  2. Policy Improvement: T·∫°o $ \pi' $ t·ª´ $ v_\pi $

### C·∫≠p nh·∫≠t ch√≠nh s√°ch:

$$
\pi(s) = \arg\max_a \sum_{s'} P(s'\mid s, a) [R(s,a,s') + \gamma V(s')]
$$

- D·ª´ng khi $ \pi $ kh√¥ng thay ƒë·ªïi n·ªØa ‚áí ch√≠nh s√°ch t·ªëi ∆∞u $ \pi^* $

---

## 4. Value Iteration (L·∫∑p gi√° tr·ªã)

### M√¥ t·∫£:
- K·∫øt h·ª£p lu√¥n b∆∞·ªõc c·∫£i ti·∫øn ch√≠nh s√°ch v√†o b∆∞·ªõc ƒë√°nh gi√°, th·ª±c hi·ªán c·∫≠p nh·∫≠t tr·ª±c ti·∫øp nh∆∞ sau:

$$
v_{k+1}(s) = \max_a \sum_{s', r} p(s', r \mid s, a) [r + \gamma v_k(s')]
\tag{4.10}
$$

- Kh√¥ng c·∫ßn ƒë√°nh gi√° ƒë·∫ßy ƒë·ªß m·ªói ch√≠nh s√°ch ‚Üí nhanh h∆°n nh∆∞ng v·∫´n h·ªôi t·ª• v·ªÅ $ v^* $

---

## 5. Asynchronous Dynamic Programming

### M√¥ t·∫£:
- Kh√¥ng c·∫ßn c·∫≠p nh·∫≠t to√†n b·ªô tr·∫°ng th√°i c√πng l√∫c.
- C·∫≠p nh·∫≠t tr·∫°ng th√°i b·∫•t k·ª≥, theo b·∫•t k·ª≥ th·ª© t·ª± n√†o.

### V√≠ d·ª• c·∫≠p nh·∫≠t t·∫°i m·ªôt tr·∫°ng th√°i:

$$
v(s_k) \leftarrow \max_a \sum_{s', r} p(s', r \mid s_k, a) [r + \gamma v(s')]
$$

- Mi·ªÖn l√† m·ªói tr·∫°ng th√°i ƒë∆∞·ª£c c·∫≠p nh·∫≠t ƒë·ªß s·ªë l·∫ßn ‚Üí v·∫´n h·ªôi t·ª•.

---

## 6. Generalized Policy Iteration (GPI)

### M√¥ h√¨nh t·ªïng qu√°t:
Hai qu√° tr√¨nh song song:
- **C·∫£i ti·∫øn ch√≠nh s√°ch**: $ \pi \leftarrow \text{greedy}(v) $
- **ƒê√°nh gi√° ch√≠nh s√°ch**: $ v \leftarrow v_\pi $

### Khi h·ªôi t·ª•:
$$
\pi = \pi^*, \quad v = v^*
$$

- ƒê√¢y l√† c∆° s·ªü c·ªßa ph·∫ßn l·ªõn thu·∫≠t to√°n RL hi·ªán ƒë·∫°i.

---

## 7. T·ªïng h·ª£p c√¥ng th·ª©c Bellman

| Lo·∫°i gi√° tr·ªã         | C√¥ng th·ª©c                                                                 |
|----------------------|---------------------------------------------------------------------------|
| $ v_\pi(s) $    | $ \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a)[r + \gamma v_\pi(s')] $ |
| $ q_\pi(s,a) $  | $ \sum_{s',r} p(s',r|s,a)[r + \gamma v_\pi(s')] $                   |
| $ v^*(s) $       | $ \max_a \sum_{s',r} p(s',r|s,a)[r + \gamma v^*(s')] $              |
| $ q^*(s,a) $     | $ \sum_{s',r} p(s',r|s,a)[r + \gamma \max_{a'} q^*(s',a')] $        |

---

## ‚úÖ K·∫øt lu·∫≠n

- DP y√™u c·∫ßu m√¥ h√¨nh m√¥i tr∆∞·ªùng ch√≠nh x√°c.
- G·ªìm c√°c thu·∫≠t to√°n: Policy Evaluation, Policy Iteration, Value Iteration, GPI.
- L√† n·ªÅn t·∫£ng cho nhi·ªÅu ph∆∞∆°ng ph√°p RL, d√π trong th·ª±c t·∫ø m√¥ h√¨nh th∆∞·ªùng kh√¥ng bi·∫øt tr∆∞·ªõc.

