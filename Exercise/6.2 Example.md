# Tính Toán True Values trong Example 6.2 Random Walk - Sutton và Barto

## Mô Tả Bài Toán Random Walk
Example 6.2 trong sách "Reinforcement Learning: An Introduction" của Sutton và Barto là một ví dụ kinh điển về random walk được sử dụng để minh họa phương pháp Temporal Difference (TD) learning. Ví dụ này trình bày cách tính toán các true values cho từng trạng thái trong một Markov Reward Process đơn giản nhưng có ý nghĩa quan trọng trong việc hiểu các nguyên lý cơ bản của reinforcement learning.

Trong random walk này, agent bắt đầu từ trạng thái trung tâm C và di chuyển sang trái hoặc phải với xác suất bằng nhau (mỗi chiều 0.5). Khi agent đạt đến biên trái (bên trái của A), episode kết thúc với reward bằng 0. Ngược lại, khi agent đạt đến biên phải (bên phải của E), episode kết thúc với reward bằng +1. Tất cả các transitions khác trong quá trình di chuyển đều có reward bằng 0.

Đây là một undiscounted task, có nghĩa là hệ số discount γ = 1, do đó không có sự giảm giá trị theo thời gian. Điều này làm cho việc tính toán true values trở nên đơn giản hơn vì chúng ta chỉ cần quan tâm đến xác suất cuối cùng sẽ kết thúc ở đâu.

## Nguyên Lý Tính Toán True Values
True value của mỗi trạng thái trong random walk này được định nghĩa là xác suất mà agent sẽ kết thúc ở bên phải (nhận reward +1) nếu bắt đầu từ trạng thái đó. Điều này có nghĩa là true value chính là xác suất thành công của việc đạt được terminal state có reward dương.

Để tính toán các true values, chúng ta có thể sử dụng nguyên lý của gambler's ruin problem, một bài toán cổ điển trong lý thuyết xác suất. Trong bối cảnh này, mỗi trạng thái có thể được coi như một vị trí trong một random walk đối xứng, và chúng ta cần tính xác suất đạt đến biên phải trước khi đạt đến biên trái.

## Phương Pháp Tính Toán Chi Tiết
### Thiết Lập Hệ Phương Trình
Gọi vπ(s) là true value của trạng thái s. Dựa trên định nghĩa và tính chất Markov của bài toán, chúng ta có thể thiết lập hệ phương trình Bellman cho từng trạng thái:

- Cho trạng thái C (trung tâm): vπ(C) = 0.5 × vπ(B) + 0.5 × vπ(D)
- Cho trạng thái B: vπ(B) = 0.5 × vπ(A) + 0.5 × vπ(C)
- Cho trạng thái D: vπ(D) = 0.5 × vπ(C) + 0.5 × vπ(E)
- Cho trạng thái A: vπ(A) = 0.5 × 0 + 0.5 × vπ(B) = 0.5 × vπ(B)
- Cho trạng thái E: vπ(E) = 0.5 × vπ(D) + 0.5 × 1 = 0.5 × vπ(D) + 0.5

### Giải Hệ Phương Trình
Từ tính đối xứng của bài toán, chúng ta có thể nhận thấy rằng random walk này có tính chất đặc biệt. Do các trạng thái được sắp xếp đối xứng và xác suất di chuyển là đều nhau, true values sẽ tăng tuyến tính từ trái sang phải.

Với năm trạng thái A, B, C, D, E và hai terminal states, chúng ta có thể tính toán trực tiếp bằng cách sử dụng công thức của gambler's ruin cho random walk đối xứng. Trong trường hợp này, với p = q = 0.5 (xác suất di chuyển trái và phải bằng nhau), true value của trạng thái i trong một random walk với N+1 khoảng cách là i/(N+1).

### Kết Quả True Values
Áp dụng công thức trên cho random walk với 5 trạng thái không kết thúc (tương đương với 6 khoảng cách từ terminal trái đến terminal phải), chúng ta có các true values như sau:

- vπ(A) = 1/6 ≈ 0.167
- vπ(B) = 2/6 ≈ 0.333
- vπ(C) = 3/6 = 0.5
- vπ(D) = 4/6 ≈ 0.667
- vπ(E) = 5/6 ≈ 0.833

## Xác Minh Kết Quả
Chúng ta có thể xác minh kết quả này bằng cách kiểm tra tính đối xứng và điều kiện biên. True value của trạng thái trung tâm C bằng 0.5 là hợp lý vì từ vị trí này, agent có cơ hội bằng nhau để đạt đến cả hai terminal states. Các trạng thái gần biên phải có true values cao hơn, trong khi các trạng thái gần biên trái có true values thấp hơn, phản ánh chính xác xác suất thành công từ mỗi vị trí.

## Ý Nghĩa và Ứng Dụng
Example 6.2 với các true values đã tính được đóng vai trò quan trọng trong việc đánh giá hiệu suất của các thuật toán học tăng cường. Các giá trị này được sử dụng làm benchmark để so sánh hiệu quả của TD(0) learning với Monte Carlo methods. Kết quả cho thấy TD(0) hội tụ nhanh hơn Monte Carlo trong bài toán này, minh chứng cho ưu điểm của phương pháp bootstrapping trong temporal difference learning.

## Kết Luận
Việc tính toán true values trong Example 6.2 thể hiện sự kết hợp giữa lý thuyết xác suất cổ điển (gambler's ruin) và các nguyên lý hiện đại của reinforcement learning. Bằng cách hiểu rõ cách tính toán các giá trị này, chúng ta có thể nắm bắt được bản chất của value functions và cách chúng phản ánh xác suất đạt được các kết quả mong muốn trong môi trường stochastic. Đây là nền tảng quan trọng để hiểu sâu hơn về các thuật toán học tăng cường phức tạp hơn.

## So Sánh Thực Nghiệm Giữa TD(0) và Monte Carlo (MC)

Ví dụ 6.2 cũng minh họa một cách thực nghiệm khả năng dự đoán của phương pháp TD(0) và Monte Carlo (MC) với hệ số học \(\alpha\) không đổi khi áp dụng vào quy trình phần thưởng Markov (Markov reward process) đã mô tả.

### Biểu Đồ Giá Trị Ước Tính (Estimated Value)

Biểu đồ bên trái cho thấy các giá trị được ước tính sau một số lượng tập (episodes) khác nhau trên một lần chạy TD(0). Các ước tính sau 100 tập rất gần với True Values. Với một tham số kích thước bước (step-size parameter) không đổi (\(\alpha = 0.1\) trong ví dụ này), các giá trị dao động không xác định để phản ứng với kết quả của các tập gần đây nhất.

### Biểu Đồ Lỗi RMS Thực Nghiệm (Empirical RMS Error)

Biểu đồ bên phải cho thấy đường cong học tập (learning curves) cho hai phương pháp (TD và MC) với các giá trị \(\alpha\) khác nhau. Thước đo hiệu suất được hiển thị là sai số căn bậc hai trung bình (root mean square - RMS) giữa hàm giá trị được học và hàm giá trị thực (true value function), được tính trung bình trên năm trạng thái, sau đó tính trung bình trên 100 lần chạy. Trong tất cả các trường hợp, hàm giá trị gần đúng ban đầu được khởi tạo bằng giá trị trung gian \(V(s) = 0.5\), cho tất cả các trạng thái.

Kết quả cho thấy phương pháp TD luôn tốt hơn phương pháp MC trong nhiệm vụ này, đặc biệt là về tốc độ hội tụ và độ ổn định của lỗi. Điều này minh chứng cho ưu điểm của bootstrapping trong TD learning, nơi mà các ước tính được cập nhật dựa trên các ước tính khác, giúp lan truyền thông tin hiệu quả hơn so với Monte Carlo chỉ dựa vào kinh nghiệm hoàn chỉnh.
